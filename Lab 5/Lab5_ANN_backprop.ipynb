{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 5 D7041E: Artificial Neural Network and Backpropagation\n",
    "\n",
    "Group 30: Sergio Serrano HernÃ¡ndez (serser-1) and Nicolas Scheidler (nicsch-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#functions of non-linear activations\n",
    "def f_sigmoid(X, deriv=False):\n",
    "    if not deriv:\n",
    "        return 1 / (1 + np.exp(-X))\n",
    "    else:\n",
    "        return f_sigmoid(X)*(1 - f_sigmoid(X))\n",
    "    \n",
    "def f_relu (X):\n",
    "    if X > 0:\n",
    "        return X\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def f_softmax(X):\n",
    "    Z = np.sum(np.exp(X), axis=1)\n",
    "    Z = Z.reshape(Z.shape[0], 1)\n",
    "    return np.exp(X) / Z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exit_with_err(err_str):\n",
    "    print >> sys.stderr, err_str\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functionality of a single hidden layer\n",
    "class Layer:\n",
    "    def __init__(self, size, batch_size, is_input=False, is_output=False,\n",
    "                 activation=f_sigmoid):\n",
    "        self.is_input = is_input\n",
    "        self.is_output = is_output\n",
    "\n",
    "        # Z is the matrix that holds output values\n",
    "        self.Z = np.zeros((batch_size, size[0]))\n",
    "        # The activation function is an externally defined function (with a\n",
    "        # derivative) that is stored here\n",
    "        self.activation = activation\n",
    "\n",
    "        # W is the outgoing weight matrix for this layer\n",
    "        self.W = None\n",
    "        # S is the matrix that holds the inputs to this layer\n",
    "        self.S = None\n",
    "        # D is the matrix that holds the deltas for this layer\n",
    "        self.D = None\n",
    "        # Fp is the matrix that holds the derivatives of the activation function\n",
    "        self.Fp = None\n",
    "\n",
    "        if not is_input:\n",
    "            self.S = np.zeros((batch_size, size[0]))\n",
    "            self.D = np.zeros((batch_size, size[0]))\n",
    "\n",
    "        if not is_output:\n",
    "            self.W = np.random.normal(size=size, scale=1E-4)\n",
    "\n",
    "        if not is_input and not is_output:\n",
    "            self.Fp = np.zeros((size[0], batch_size))\n",
    "\n",
    "    def forward_propagate(self):\n",
    "        if self.is_input:\n",
    "            return self.Z.dot(self.W)\n",
    "\n",
    "        self.Z = self.activation(self.S)\n",
    "        if self.is_output:\n",
    "            return self.Z\n",
    "        else:\n",
    "            # For hidden layers, we add the bias values here\n",
    "            self.Z = np.append(self.Z, np.ones((self.Z.shape[0], 1)), axis=1)\n",
    "            self.Fp = self.activation(self.S, deriv=True).T\n",
    "            return self.Z.dot(self.W)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLayerPerceptron:\n",
    "    def __init__(self, layer_config, batch_size=100):\n",
    "        self.layers = []\n",
    "        self.num_layers = len(layer_config)\n",
    "        self.minibatch_size = batch_size\n",
    "\n",
    "        for i in range(self.num_layers-1):\n",
    "            if i == 0:\n",
    "                print (\"Initializing input layer with size {0}.\".format(layer_config[i]))\n",
    "                # Here, we add an additional unit at the input for the bias\n",
    "                # weight.\n",
    "                self.layers.append(Layer([layer_config[i]+1, layer_config[i+1]],\n",
    "                                         batch_size,\n",
    "                                         is_input=True))\n",
    "            else:\n",
    "                print (\"Initializing hidden layer with size {0}.\".format(layer_config[i]))\n",
    "                # Here we add an additional unit in the hidden layers for the\n",
    "                # bias weight.\n",
    "                self.layers.append(Layer([layer_config[i]+1, layer_config[i+1]],\n",
    "                                         batch_size,\n",
    "                                         activation=f_sigmoid))\n",
    "\n",
    "        print (\"Initializing output layer with size {0}.\".format(layer_config[-1]))\n",
    "        self.layers.append(Layer([layer_config[-1], None],\n",
    "                                 batch_size,\n",
    "                                 is_output=True,\n",
    "                                 activation=f_softmax))\n",
    "        print (\"Done!\")\n",
    "\n",
    "    def forward_propagate(self, data):\n",
    "        # We need to be sure to add bias values to the input\n",
    "        self.layers[0].Z = np.append(data, np.ones((data.shape[0], 1)), axis=1)\n",
    "\n",
    "        for i in range(self.num_layers-1):\n",
    "            self.layers[i+1].S = self.layers[i].forward_propagate()\n",
    "        return self.layers[-1].forward_propagate()\n",
    "\n",
    "    def backpropagate(self, yhat, labels):\n",
    "        \n",
    "        #exit_with_err(\"FIND ME IN THE CODE, What is computed in the next line of code?\\n\")\n",
    "\n",
    "        self.layers[-1].D = (yhat - labels).T\n",
    "        for i in range(self.num_layers-2, 0, -1):\n",
    "            # We do not calculate deltas for the bias values\n",
    "            W_nobias = self.layers[i].W[0:-1, :]\n",
    "            \n",
    "            #exit_with_err(\"FIND ME IN THE CODE, What does this 'for' loop do?\\n\")\n",
    "            \n",
    "            \n",
    "            self.layers[i].D = W_nobias.dot(self.layers[i+1].D) * self.layers[i].Fp\n",
    "\n",
    "    def update_weights(self, eta):\n",
    "        for i in range(0, self.num_layers-1):\n",
    "            W_grad = -eta*(self.layers[i+1].D.dot(self.layers[i].Z)).T\n",
    "            self.layers[i].W += W_grad\n",
    "\n",
    "    def evaluate(self, train_data, train_labels, test_data, test_labels,\n",
    "                 num_epochs=70, eta=0.05, eval_train=False, eval_test=True):\n",
    "\n",
    "        N_train = len(train_labels)*len(train_labels[0])\n",
    "        N_test = len(test_labels)*len(test_labels[0])\n",
    "\n",
    "        #print (\"Training for {0} epochs...\".format(num_epochs))\n",
    "        for t in range(0, num_epochs):\n",
    "            out_str = \"[{0:4d}] \".format(t)\n",
    "\n",
    "            for b_data, b_labels in zip(train_data, train_labels):\n",
    "                output = self.forward_propagate(b_data)\n",
    "                self.backpropagate(output, b_labels)\n",
    "                \n",
    "                #exit_with_err(\"FIND ME IN THE CODE, How does weight update is implemented? What is eta?\\n\")\n",
    "\n",
    "                self.update_weights(eta=eta)\n",
    "\n",
    "            if eval_train:\n",
    "                errs = 0\n",
    "                for b_data, b_labels in zip(train_data, train_labels):\n",
    "                    output = self.forward_propagate(b_data)\n",
    "                    yhat = np.argmax(output, axis=1)\n",
    "                    errs += np.sum(1-b_labels[np.arange(len(b_labels)), yhat])\n",
    "\n",
    "                out_str = (\"{0} Training error: {1:.5f}\".format(out_str,\n",
    "                                                           float(errs)/N_train))\n",
    "\n",
    "            if eval_test:\n",
    "                errs = 0\n",
    "                for b_data, b_labels in zip(test_data, test_labels):\n",
    "                    output = self.forward_propagate(b_data)\n",
    "                    yhat = np.argmax(output, axis=1)\n",
    "                    errs += np.sum(1-b_labels[np.arange(len(b_labels)), yhat])\n",
    "                out_str = (\"{0} Test error: {1:.5f}\").format(out_str,\n",
    "                                                       float(errs)/N_test)\n",
    "\n",
    "            #print (out_str)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1.\n",
    "#### ***Answers to the questions of the code:***\n",
    "1. The line `self.layers[-1].D = (yhat - labels).T` calculates the error in the last layer which will be backpropagated to calculate all the Deltas in the previous laters. 'yhat' is the prediction made by the network and the 'labels' are the correct instances.\n",
    "\n",
    "2. \n",
    "        self.layers[-1].D = (yhat - labels).T\n",
    "        for i in range(self.num_layers-2, 0, -1):\n",
    "            # We do not calculate deltas for the bias values\n",
    "            \n",
    "            W_nobias = self.layers[i].W[0:-1, :]\n",
    "            self.layers[i].D = W_nobias.dot(self.layers[i+1].D) * self.layers[i].Fp\n",
    "    The purpose of this loop is to calculate the deltas for each hidden layer, excluding the input layer. These deltas are then used to update the weights during the weight update step in the backpropagation algorithm.\n",
    "\n",
    "3. How is the weight update implemented? What is eta?\n",
    "The weight update is implemented through a function \"update_weights\" that calculates the gradient of each layer using a for loop iterating over each layer, multiplying its output by the delta error of the next layer and by \"eta\", which is the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_to_bit_vector(labels, nbits):\n",
    "    bit_vector = np.zeros((labels.shape[0], nbits))\n",
    "    for i in range(labels.shape[0]):\n",
    "        bit_vector[i, labels[i]] = 1.0\n",
    "\n",
    "    return bit_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_batches(data, labels, batch_size, create_bit_vector=False):\n",
    "    N = data.shape[0]\n",
    "    print (\"Batch size {0}, the number of examples {1}.\".format(batch_size,N))\n",
    "\n",
    "    if N % batch_size != 0:\n",
    "        print (\"Warning in create_minibatches(): Batch size {0} does not \" \\\n",
    "              \"evenly divide the number of examples {1}.\".format(batch_size,N))\n",
    "    chunked_data = []\n",
    "    chunked_labels = []\n",
    "    idx = 0\n",
    "    while idx + batch_size <= N:\n",
    "        chunked_data.append(data[idx:idx+batch_size, :])\n",
    "        if not create_bit_vector:\n",
    "            chunked_labels.append(labels[idx:idx+batch_size])\n",
    "        else:\n",
    "            bit_vector = label_to_bit_vector(labels[idx:idx+batch_size], 10)\n",
    "            chunked_labels.append(bit_vector)\n",
    "\n",
    "        idx += batch_size\n",
    "\n",
    "    return chunked_data, chunked_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_backprop(batch_size, Train_images, Train_labels, Valid_images, Valid_labels):\n",
    "    \n",
    "    print (\"Creating data...\")\n",
    "    batched_train_data, batched_train_labels = create_batches(Train_images, Train_labels,\n",
    "                                              batch_size,\n",
    "                                              create_bit_vector=True)\n",
    "    batched_valid_data, batched_valid_labels = create_batches(Valid_images, Valid_labels,\n",
    "                                              batch_size,\n",
    "                                              create_bit_vector=True)\n",
    "    print (\"Done!\")\n",
    "\n",
    "\n",
    "    return batched_train_data, batched_train_labels,  batched_valid_data, batched_valid_labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "(Xtr, Ltr), (X_test, L_test)=mnist.load_data()\n",
    "\n",
    "Xtr = Xtr.reshape(60000, 784)\n",
    "X_test = X_test.reshape(10000, 784)\n",
    "Xtr = Xtr.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "Xtr /= 255\n",
    "X_test /= 255\n",
    "print(Xtr.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating data...\n",
      "Batch size 100, the number of examples 60000.\n",
      "Batch size 100, the number of examples 10000.\n",
      "Done!\n",
      "Initializing input layer with size 784.\n",
      "Initializing hidden layer with size 100.\n",
      "Initializing hidden layer with size 100.\n",
      "Initializing output layer with size 10.\n",
      "Done!\n",
      "Training for 70 epochs...\n",
      "[   0]  Training error: 0.73552 Test error: 0.72000\n",
      "[   1]  Training error: 0.08963 Test error: 0.08910\n",
      "[   2]  Training error: 0.05932 Test error: 0.06110\n",
      "[   3]  Training error: 0.04727 Test error: 0.05160\n",
      "[   4]  Training error: 0.04153 Test error: 0.04770\n",
      "[   5]  Training error: 0.03540 Test error: 0.04290\n",
      "[   6]  Training error: 0.03230 Test error: 0.04060\n",
      "[   7]  Training error: 0.02750 Test error: 0.03740\n",
      "[   8]  Training error: 0.02318 Test error: 0.03580\n",
      "[   9]  Training error: 0.02068 Test error: 0.03350\n",
      "[  10]  Training error: 0.01968 Test error: 0.03520\n",
      "[  11]  Training error: 0.01933 Test error: 0.03350\n",
      "[  12]  Training error: 0.01695 Test error: 0.03250\n",
      "[  13]  Training error: 0.01668 Test error: 0.03210\n",
      "[  14]  Training error: 0.02027 Test error: 0.03550\n",
      "[  15]  Training error: 0.01318 Test error: 0.03380\n",
      "[  16]  Training error: 0.01235 Test error: 0.03150\n",
      "[  17]  Training error: 0.01405 Test error: 0.03320\n",
      "[  18]  Training error: 0.01513 Test error: 0.03210\n",
      "[  19]  Training error: 0.01195 Test error: 0.03170\n",
      "[  20]  Training error: 0.01092 Test error: 0.03130\n",
      "[  21]  Training error: 0.01153 Test error: 0.03180\n",
      "[  22]  Training error: 0.00987 Test error: 0.03190\n",
      "[  23]  Training error: 0.00915 Test error: 0.03200\n",
      "[  24]  Training error: 0.00973 Test error: 0.03120\n",
      "[  25]  Training error: 0.00877 Test error: 0.02800\n",
      "[  26]  Training error: 0.00710 Test error: 0.02920\n",
      "[  27]  Training error: 0.00738 Test error: 0.03090\n",
      "[  28]  Training error: 0.00415 Test error: 0.02840\n",
      "[  29]  Training error: 0.00842 Test error: 0.03180\n",
      "[  30]  Training error: 0.00410 Test error: 0.02930\n",
      "[  31]  Training error: 0.00350 Test error: 0.02910\n",
      "[  32]  Training error: 0.00252 Test error: 0.02870\n",
      "[  33]  Training error: 0.01137 Test error: 0.03320\n",
      "[  34]  Training error: 0.00740 Test error: 0.03040\n",
      "[  35]  Training error: 0.00572 Test error: 0.02980\n",
      "[  36]  Training error: 0.00512 Test error: 0.02920\n",
      "[  37]  Training error: 0.00483 Test error: 0.03020\n",
      "[  38]  Training error: 0.00608 Test error: 0.02940\n",
      "[  39]  Training error: 0.00370 Test error: 0.02980\n",
      "[  40]  Training error: 0.00345 Test error: 0.02950\n",
      "[  41]  Training error: 0.00265 Test error: 0.02730\n",
      "[  42]  Training error: 0.00157 Test error: 0.02630\n",
      "[  43]  Training error: 0.00158 Test error: 0.02680\n",
      "[  44]  Training error: 0.00585 Test error: 0.02970\n",
      "[  45]  Training error: 0.00408 Test error: 0.02690\n",
      "[  46]  Training error: 0.00338 Test error: 0.02800\n",
      "[  47]  Training error: 0.00187 Test error: 0.02730\n",
      "[  48]  Training error: 0.00300 Test error: 0.02750\n",
      "[  49]  Training error: 0.00073 Test error: 0.02590\n",
      "[  50]  Training error: 0.00082 Test error: 0.02640\n",
      "[  51]  Training error: 0.00023 Test error: 0.02490\n",
      "[  52]  Training error: 0.00008 Test error: 0.02520\n",
      "[  53]  Training error: 0.00003 Test error: 0.02510\n",
      "[  54]  Training error: 0.00003 Test error: 0.02510\n",
      "[  55]  Training error: 0.00003 Test error: 0.02460\n",
      "[  56]  Training error: 0.00003 Test error: 0.02480\n",
      "[  57]  Training error: 0.00002 Test error: 0.02520\n",
      "[  58]  Training error: 0.00002 Test error: 0.02530\n",
      "[  59]  Training error: 0.00002 Test error: 0.02530\n",
      "[  60]  Training error: 0.00002 Test error: 0.02510\n",
      "[  61]  Training error: 0.00002 Test error: 0.02510\n",
      "[  62]  Training error: 0.00002 Test error: 0.02490\n",
      "[  63]  Training error: 0.00002 Test error: 0.02490\n",
      "[  64]  Training error: 0.00002 Test error: 0.02490\n",
      "[  65]  Training error: 0.00002 Test error: 0.02500\n",
      "[  66]  Training error: 0.00002 Test error: 0.02510\n",
      "[  67]  Training error: 0.00002 Test error: 0.02530\n",
      "[  68]  Training error: 0.00000 Test error: 0.02510\n",
      "[  69]  Training error: 0.00002 Test error: 0.02520\n",
      "Done:)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "batch_size=100;\n",
    "\n",
    "train_data, train_labels, valid_data, valid_labels=prepare_for_backprop(batch_size, Xtr, Ltr, X_test, L_test)\n",
    "\n",
    "mlp = MultiLayerPerceptron(layer_config=[784, 100, 100, 10], batch_size=batch_size)\n",
    "\n",
    "mlp.evaluate(train_data, train_labels, valid_data, valid_labels,\n",
    "             eval_train=True)\n",
    "\n",
    "print(\"Done:)\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: Run the code with the suggested configuration of the hyperparameters: number of epochs = 70 and learning rate = 0.05. What is the classification accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9748\n"
     ]
    }
   ],
   "source": [
    "output = mlp.forward_propagate(X_test)\n",
    "y_hat = np.argmax(output, axis = 1)\n",
    "accuracy = np.sum(y_hat == L_test)/len(L_test)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classification accuracy when training the MLP for 70 epochs witha a learning rate of 0.05 is 97.48%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3: Run the code with Learning rate =0.005 and Learning rate =0.5. Explain the observed differences in the functionality of the multi-layer perceptron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing input layer with size 784.\n",
      "Initializing hidden layer with size 100.\n",
      "Initializing hidden layer with size 100.\n",
      "Initializing output layer with size 10.\n",
      "Done!\n",
      "The Accuracy when training with a learning rate of 0.005  is: 0.9749\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sexysehe\\AppData\\Local\\Temp\\ipykernel_22028\\3670988153.py:4: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-X))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Accuracy when training with a learning rate of 0.5  is: 0.0974\n"
     ]
    }
   ],
   "source": [
    "mlp = MultiLayerPerceptron(layer_config=[784, 100, 100, 10], batch_size=batch_size)\n",
    "\n",
    "learning_rates = [0.005, 0.5]\n",
    "\n",
    "for i in range(2):  \n",
    "    mlp.evaluate(train_data, train_labels, valid_data, valid_labels, num_epochs=70, eta=learning_rates[i], eval_train=False, eval_test=True);  \n",
    "    output = mlp.forward_propagate(X_test)\n",
    "    y_hat = np.argmax(output, axis = 1)\n",
    "    accuracy = np.sum(y_hat == L_test)/len(L_test)\n",
    "    print(f\"The Accuracy when training with a learning rate of {learning_rates[i]}  is: {accuracy:.4f}\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see by the accuracies gotten for the different learning rates, increasing it to 0.5 has made the accuracy be much lower when testing the MLP. This could be because since the learning rate is so large it makes the optimization process jump around the minima."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4 \n",
    "#### Extend the code implementing the ReLU output function. Run the perceptron with the suggested by default configuration of hyperparameters: number of epochs = 70 and learning rate =0.05. What is the classification accuracy?\n",
    "\n",
    "We will first change the parts in the object oriented programming that are affected by this, you can see how below we change the activations from sigmoids to the relu that we have defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_relu(X, deriv=False):\n",
    "    if not deriv:\n",
    "        return np.maximum(0, X)\n",
    "    else:\n",
    "        return np.where(X > 0, 1, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functionality of a single hidden layer\n",
    "class Layer:\n",
    "    def __init__(self, size, batch_size, is_input=False, is_output=False,\n",
    "                 activation=f_relu):\n",
    "        self.is_input = is_input\n",
    "        self.is_output = is_output\n",
    "\n",
    "        # Z is the matrix that holds output values\n",
    "        self.Z = np.zeros((batch_size, size[0]))\n",
    "        # The activation function is an externally defined function (with a\n",
    "        # derivative) that is stored here\n",
    "        self.activation = activation\n",
    "\n",
    "        # W is the outgoing weight matrix for this layer\n",
    "        self.W = None\n",
    "        # S is the matrix that holds the inputs to this layer\n",
    "        self.S = None\n",
    "        # D is the matrix that holds the deltas for this layer\n",
    "        self.D = None\n",
    "        # Fp is the matrix that holds the derivatives of the activation function\n",
    "        self.Fp = None\n",
    "\n",
    "        if not is_input:\n",
    "            self.S = np.zeros((batch_size, size[0]))\n",
    "            self.D = np.zeros((batch_size, size[0]))\n",
    "\n",
    "        if not is_output:\n",
    "            self.W = np.random.normal(size=size, scale=1E-4)\n",
    "\n",
    "        if not is_input and not is_output:\n",
    "            self.Fp = np.zeros((size[0], batch_size))\n",
    "\n",
    "    def forward_propagate(self):\n",
    "        if self.is_input:\n",
    "            return self.Z.dot(self.W)\n",
    "\n",
    "        self.Z = self.activation(self.S)\n",
    "        if self.is_output:\n",
    "            return self.Z\n",
    "        else:\n",
    "            # For hidden layers, we add the bias values here\n",
    "            self.Z = np.append(self.Z, np.ones((self.Z.shape[0], 1)), axis=1)\n",
    "            self.Fp = self.activation(self.S, deriv=True).T\n",
    "            return self.Z.dot(self.W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLayerPerceptron:\n",
    "    def __init__(self, layer_config, batch_size=100):\n",
    "        self.layers = []\n",
    "        self.num_layers = len(layer_config)\n",
    "        self.minibatch_size = batch_size\n",
    "\n",
    "        for i in range(self.num_layers-1):\n",
    "            if i == 0:\n",
    "                print (\"Initializing input layer with size {0}.\".format(layer_config[i]))\n",
    "                # Here, we add an additional unit at the input for the bias\n",
    "                # weight.\n",
    "                self.layers.append(Layer([layer_config[i]+1, layer_config[i+1]],\n",
    "                                         batch_size,\n",
    "                                         is_input=True))\n",
    "            else:\n",
    "                print (\"Initializing hidden layer with size {0}.\".format(layer_config[i]))\n",
    "                # Here we add an additional unit in the hidden layers for the\n",
    "                # bias weight.\n",
    "                self.layers.append(Layer([layer_config[i]+1, layer_config[i+1]],\n",
    "                                         batch_size,\n",
    "                                         activation=f_relu))\n",
    "\n",
    "        print (\"Initializing output layer with size {0}.\".format(layer_config[-1]))\n",
    "        self.layers.append(Layer([layer_config[-1], None],\n",
    "                                 batch_size,\n",
    "                                 is_output=True,\n",
    "                                 activation=f_softmax))\n",
    "        print (\"Done!\")\n",
    "\n",
    "    def forward_propagate(self, data):\n",
    "        # We need to be sure to add bias values to the input\n",
    "        self.layers[0].Z = np.append(data, np.ones((data.shape[0], 1)), axis=1)\n",
    "\n",
    "        for i in range(self.num_layers-1):\n",
    "            self.layers[i+1].S = self.layers[i].forward_propagate()\n",
    "        return self.layers[-1].forward_propagate()\n",
    "\n",
    "    def backpropagate(self, yhat, labels):\n",
    "        \n",
    "        #exit_with_err(\"FIND ME IN THE CODE, What is computed in the next line of code?\\n\")\n",
    "\n",
    "        self.layers[-1].D = (yhat - labels).T\n",
    "        for i in range(self.num_layers-2, 0, -1):\n",
    "            # We do not calculate deltas for the bias values\n",
    "            W_nobias = self.layers[i].W[0:-1, :]\n",
    "            \n",
    "            #exit_with_err(\"FIND ME IN THE CODE, What does this 'for' loop do?\\n\")\n",
    "            \n",
    "            \n",
    "            self.layers[i].D = W_nobias.dot(self.layers[i+1].D) * self.layers[i].Fp\n",
    "\n",
    "    def update_weights(self, eta):\n",
    "        for i in range(0, self.num_layers-1):\n",
    "            W_grad = -eta*(self.layers[i+1].D.dot(self.layers[i].Z)).T\n",
    "            self.layers[i].W += W_grad\n",
    "\n",
    "    def evaluate(self, train_data, train_labels, test_data, test_labels,\n",
    "                 num_epochs=70, eta=0.05, eval_train=False, eval_test=True):\n",
    "\n",
    "        N_train = len(train_labels)*len(train_labels[0])\n",
    "        N_test = len(test_labels)*len(test_labels[0])\n",
    "\n",
    "        #print (\"Training for {0} epochs...\".format(num_epochs))\n",
    "        for t in range(0, num_epochs):\n",
    "            out_str = \"[{0:4d}] \".format(t)\n",
    "\n",
    "            for b_data, b_labels in zip(train_data, train_labels):\n",
    "                output = self.forward_propagate(b_data)\n",
    "                self.backpropagate(output, b_labels)\n",
    "                \n",
    "                #exit_with_err(\"FIND ME IN THE CODE, How does weight update is implemented? What is eta?\\n\")\n",
    "\n",
    "                self.update_weights(eta=eta)\n",
    "\n",
    "            if eval_train:\n",
    "                errs = 0\n",
    "                for b_data, b_labels in zip(train_data, train_labels):\n",
    "                    output = self.forward_propagate(b_data)\n",
    "                    yhat = np.argmax(output, axis=1)\n",
    "                    errs += np.sum(1-b_labels[np.arange(len(b_labels)), yhat])\n",
    "\n",
    "                out_str = (\"{0} Training error: {1:.5f}\".format(out_str,\n",
    "                                                           float(errs)/N_train))\n",
    "\n",
    "            if eval_test:\n",
    "                errs = 0\n",
    "                for b_data, b_labels in zip(test_data, test_labels):\n",
    "                    output = self.forward_propagate(b_data)\n",
    "                    yhat = np.argmax(output, axis=1)\n",
    "                    errs += np.sum(1-b_labels[np.arange(len(b_labels)), yhat])\n",
    "                out_str = (\"{0} Test error: {1:.5f}\").format(out_str,\n",
    "                                                       float(errs)/N_test)\n",
    "\n",
    "            print (out_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we will train the MLP using relu activation function, by not initializing the learning rate it stays at the default value of 0.05 which is the one required by this task, and the same goes with the number of epochs which by default is 70."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing input layer with size 784.\n",
      "Initializing hidden layer with size 100.\n",
      "Initializing hidden layer with size 100.\n",
      "Initializing output layer with size 10.\n",
      "Done!\n",
      "[   0]  Training error: 0.90137 Test error: 0.90420\n",
      "[   1]  Training error: 0.90137 Test error: 0.90420\n",
      "[   2]  Training error: 0.90137 Test error: 0.90420\n",
      "[   3]  Training error: 0.90137 Test error: 0.90420\n",
      "[   4]  Training error: 0.90137 Test error: 0.90420\n",
      "[   5]  Training error: 0.90137 Test error: 0.90420\n",
      "[   6]  Training error: 0.90137 Test error: 0.90420\n",
      "[   7]  Training error: 0.90137 Test error: 0.90420\n",
      "[   8]  Training error: 0.90137 Test error: 0.90420\n",
      "[   9]  Training error: 0.90137 Test error: 0.90420\n",
      "[  10]  Training error: 0.90137 Test error: 0.90420\n",
      "[  11]  Training error: 0.90137 Test error: 0.90420\n",
      "[  12]  Training error: 0.90137 Test error: 0.90420\n",
      "[  13]  Training error: 0.90137 Test error: 0.90420\n",
      "[  14]  Training error: 0.90137 Test error: 0.90420\n",
      "[  15]  Training error: 0.90137 Test error: 0.90420\n",
      "[  16]  Training error: 0.90137 Test error: 0.90420\n",
      "[  17]  Training error: 0.90137 Test error: 0.90420\n",
      "[  18]  Training error: 0.90137 Test error: 0.90420\n",
      "[  19]  Training error: 0.90137 Test error: 0.90420\n",
      "[  20]  Training error: 0.90137 Test error: 0.90420\n",
      "[  21]  Training error: 0.90137 Test error: 0.90420\n",
      "[  22]  Training error: 0.90137 Test error: 0.90420\n",
      "[  23]  Training error: 0.90137 Test error: 0.90420\n",
      "[  24]  Training error: 0.90137 Test error: 0.90420\n",
      "[  25]  Training error: 0.90137 Test error: 0.90420\n",
      "[  26]  Training error: 0.90137 Test error: 0.90420\n",
      "[  27]  Training error: 0.90137 Test error: 0.90420\n",
      "[  28]  Training error: 0.90137 Test error: 0.90420\n",
      "[  29]  Training error: 0.90137 Test error: 0.90420\n",
      "[  30]  Training error: 0.90137 Test error: 0.90420\n",
      "[  31]  Training error: 0.90137 Test error: 0.90420\n",
      "[  32]  Training error: 0.90137 Test error: 0.90420\n",
      "[  33]  Training error: 0.90137 Test error: 0.90420\n",
      "[  34]  Training error: 0.90137 Test error: 0.90420\n",
      "[  35]  Training error: 0.90137 Test error: 0.90420\n",
      "[  36]  Training error: 0.90137 Test error: 0.90420\n",
      "[  37]  Training error: 0.90137 Test error: 0.90420\n",
      "[  38]  Training error: 0.90137 Test error: 0.90420\n",
      "[  39]  Training error: 0.90137 Test error: 0.90420\n",
      "[  40]  Training error: 0.90137 Test error: 0.90420\n",
      "[  41]  Training error: 0.90137 Test error: 0.90420\n",
      "[  42]  Training error: 0.90137 Test error: 0.90420\n",
      "[  43]  Training error: 0.90137 Test error: 0.90420\n",
      "[  44]  Training error: 0.90137 Test error: 0.90420\n",
      "[  45]  Training error: 0.90137 Test error: 0.90420\n",
      "[  46]  Training error: 0.90137 Test error: 0.90420\n",
      "[  47]  Training error: 0.90137 Test error: 0.90420\n",
      "[  48]  Training error: 0.90137 Test error: 0.90420\n",
      "[  49]  Training error: 0.90137 Test error: 0.90420\n",
      "[  50]  Training error: 0.90137 Test error: 0.90420\n",
      "[  51]  Training error: 0.90137 Test error: 0.90420\n",
      "[  52]  Training error: 0.90137 Test error: 0.90420\n",
      "[  53]  Training error: 0.90137 Test error: 0.90420\n",
      "[  54]  Training error: 0.90137 Test error: 0.90420\n",
      "[  55]  Training error: 0.90137 Test error: 0.90420\n",
      "[  56]  Training error: 0.90137 Test error: 0.90420\n",
      "[  57]  Training error: 0.90137 Test error: 0.90420\n",
      "[  58]  Training error: 0.90137 Test error: 0.90420\n",
      "[  59]  Training error: 0.90137 Test error: 0.90420\n",
      "[  60]  Training error: 0.90137 Test error: 0.90420\n",
      "[  61]  Training error: 0.90137 Test error: 0.90420\n",
      "[  62]  Training error: 0.90137 Test error: 0.90420\n",
      "[  63]  Training error: 0.90137 Test error: 0.90420\n",
      "[  64]  Training error: 0.90137 Test error: 0.90420\n",
      "[  65]  Training error: 0.90137 Test error: 0.90420\n",
      "[  66]  Training error: 0.90137 Test error: 0.90420\n",
      "[  67]  Training error: 0.90137 Test error: 0.90420\n",
      "[  68]  Training error: 0.90137 Test error: 0.90420\n",
      "[  69]  Training error: 0.90137 Test error: 0.90420\n",
      "Done:)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mlp_relu = MultiLayerPerceptron(layer_config=[784, 100, 100, 10], batch_size=batch_size)\n",
    "\n",
    "mlp_relu.evaluate(train_data, train_labels, valid_data, valid_labels,\n",
    "             eval_train=True)\n",
    "\n",
    "print(\"Done:)\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.0958\n"
     ]
    }
   ],
   "source": [
    "output = mlp_relu.forward_propagate(X_test)\n",
    "y_hat = np.argmax(output, axis = 1)\n",
    "accuracy = np.sum(y_hat == L_test)/len(L_test)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The accuracy of the mlp using relu is much lower to the one using the sigmoid activation function, giving us a value of 9.58% which is the final required result of this Lab"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
